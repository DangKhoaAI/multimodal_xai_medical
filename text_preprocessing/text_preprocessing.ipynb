{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing\n",
    "\n",
    "This notebook preprocesses the Indiana University Chest X-ray Collection's text metadata in order to use within the multimodal. The text data is already preprocessed and ready to use with ground truth labels as another column in the same data directory of this repo.\n",
    "\n",
    "This preprocessing is very standard and doesn't contain any special operation.\n",
    "\n",
    "Apart from tokenization and removing stopwords, these tokens are converted to padded - sequences, in order to be ingested into the 1-D CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(doc):\n",
    "    tokens = doc.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean_medical(text_list):\n",
    "    text_list = [single_string.lower().strip() for single_string in text_list] # lower case & whitespace removal\n",
    "    text_list = [re.sub(r'\\d+', '', single_string) for single_string in text_list] # remove numerics\n",
    "    text_list = [single_string.translate(str.maketrans(\"\",\"\",string.punctuation)) for single_string in text_list] # remove punctuation \n",
    "    text_list = [tokenize(single_string) for single_string in text_list]\n",
    "    return text_list\n",
    "\n",
    "def list_to_seq(text_list, num_words, seq_len):\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(text_list)\n",
    "    sequences = tokenizer.texts_to_sequences(text_list)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=seq_len, padding='post')\n",
    "    return padded_sequences,tokenizer.word_index\n",
    "\n",
    "def make_processed_text(in_path, filename):\n",
    "    # read in \n",
    "    data = pd.read_csv(in_path, index_col=0)\n",
    "    clean_data = clean_medical(list(data.Text))\n",
    "    data['Text'] = clean_data\n",
    "    print('example document: {}'.format(clean_data[0])) # example document\n",
    "    seq_data, vocab = list_to_seq(text_list=clean_data, num_words=15000, seq_len=140) # on average 40 words per document, keeping it a bit more then that \n",
    "    print('corresponding padded sequence of the example document: {}'.format(seq_data[0])) # corresponding padded sequence of the example document\n",
    "    data = dict(zip(list(data['ID']),seq_data))\n",
    "    with open('../data/'+ filename + '.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f, pickle.HIGHEST_PROTOCOL)\n",
    "    with open('../data/vocab.json', 'w') as fp:\n",
    "        json.dump(vocab, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example document: xxxx normal chest heart size normal lungs clear xxxx normal pneumonia effusions edema pneumothorax adenopathy nodules masses\n",
      "corresponding padded sequence of the example document: [  1   2  14   6   9   2   8  10   1   2  70  44  59   5 179  96 120   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "make_processed_text('../data/ids_raw_texts_labels.csv', 'text_processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
